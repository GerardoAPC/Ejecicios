{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN0VeMo/un8mt+qz7/xTzQt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"cUPKXPipIHAI"}},{"cell_type":"code","source":["!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n","!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n","!pip install -q findspark"],"metadata":{"id":"KwhCm-K-Ii19","executionInfo":{"status":"ok","timestamp":1680307599361,"user_tz":300,"elapsed":33836,"user":{"displayName":"Gerardo Antonio Perez Clavijo","userId":"15319807081413932697"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""],"metadata":{"id":"XN8vW_E-Iise","executionInfo":{"status":"ok","timestamp":1680307621050,"user_tz":300,"elapsed":168,"user":{"displayName":"Gerardo Antonio Perez Clavijo","userId":"15319807081413932697"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Propiedad utilizada para formatear mejor las tablas de salida\n","spark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"id":"R-sqHeWPIihu","executionInfo":{"status":"ok","timestamp":1680307631118,"user_tz":300,"elapsed":8531,"user":{"displayName":"Gerardo Antonio Perez Clavijo","userId":"15319807081413932697"}},"outputId":"b9c51771-172b-4261-ab9e-5c269e981e7f"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7f4834605cd0>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://b67bb834938e:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.1.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":[],"metadata":{"id":"_eB-vwnzIiXX"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9qCfJP0pIGQs","executionInfo":{"status":"ok","timestamp":1680307782395,"user_tz":300,"elapsed":1817,"user":{"displayName":"Gerardo Antonio Perez Clavijo","userId":"15319807081413932697"}},"outputId":"45f06f35-d968-4b3e-a1d1-ef5b05f8fc7d"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+--------+-------+-----+\n","|firstname|lastname|country|state|\n","+---------+--------+-------+-----+\n","|    James|   Smith|    USA|   CA|\n","|  Michael|    Rose|    USA|   NY|\n","|   Robert|Williams|    USA|   CA|\n","|    Maria|   Jones|    USA|   FL|\n","+---------+--------+-------+-----+\n","\n","**********collect**********\n","[Row(firstname='James', lastname='Smith', country='USA', state='CA'), Row(firstname='Michael', lastname='Rose', country='USA', state='NY'), Row(firstname='Robert', lastname='Williams', country='USA', state='CA'), Row(firstname='Maria', lastname='Jones', country='USA', state='FL')]\n","['CA', 'NY', 'CA', 'FL']\n","['CA', 'NY', 'FL']\n","['CA', 'NY', 'CA', 'FL']\n","[Row(state='CA'), Row(state='NY'), Row(state='CA'), Row(state='FL')]\n","['CA', 'NY', 'CA', 'FL']\n","['CA', 'NY', 'CA', 'FL']\n","['CA', 'NY', 'CA', 'FL']\n","['James', 'Michael', 'Robert', 'Maria']\n"]}],"source":["data = [(\"James\",\"Smith\",\"USA\",\"CA\"),(\"Michael\",\"Rose\",\"USA\",\"NY\"), \\\n","    (\"Robert\",\"Williams\",\"USA\",\"CA\"),(\"Maria\",\"Jones\",\"USA\",\"FL\") \\\n","  ]\n","columns=[\"firstname\",\"lastname\",\"country\",\"state\"]\n","df=spark.createDataFrame(data=data,schema=columns)\n","df.show()\n","print(\"**********collect**********\")\n","print(df.collect())\n","\n","states1=df.rdd.map(lambda x: x[3]).collect()\n","print(states1)\n","#['CA', 'NY', 'CA', 'FL']\n","from collections import OrderedDict \n","res = list(OrderedDict.fromkeys(states1)) \n","print(res)\n","#['CA', 'NY', 'FL']\n","\n","\n","#Example 2\n","states2=df.rdd.map(lambda x: x.state).collect()\n","print(states2)\n","#['CA', 'NY', 'CA', 'FL']\n","\n","states3=df.select(df.state).collect()\n","print(states3)\n","#[Row(state='CA'), Row(state='NY'), Row(state='CA'), Row(state='FL')]\n","\n","states4=df.select(df.state).rdd.flatMap(lambda x: x).collect()\n","print(states4)\n","#['CA', 'NY', 'CA', 'FL']\n","\n","states5=df.select(df.state).toPandas()['state']\n","states6=list(states5)\n","print(states6)\n","#['CA', 'NY', 'CA', 'FL']\n","\n","pandDF=df.select(df.state,df.firstname).toPandas()\n","print(list(pandDF['state']))\n","print(list(pandDF['firstname']))"]},{"cell_type":"code","source":[],"metadata":{"id":"TBF7b5RKIV2H"},"execution_count":null,"outputs":[]}]}